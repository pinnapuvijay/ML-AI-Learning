# Decision Trees for Predicting Exam Success: A Complete Guide with Metrics and Pruning

## Introduction

Decision trees are powerful classification tools that create intuitive, tree-like structures for making predictions. In this article, we'll explore how decision trees can predict exam success based on study hours, visualize the decision-making process, evaluate model performance using precision and recall metrics, and demonstrate how pruning techniques can produce more robust models.

## The Dataset

Let's consider a dataset of 50 students with their study hours and final exam results (pass/fail, with passing defined as a score of 75 or higher):

| Student ID | Hours Studied | Exam Score | Pass/Fail |
|------------|---------------|------------|-----------|
| 1          | 2.5           | 68         | Fail      |
| 2          | 7.0           | 89         | Pass      |
| 3          | 4.5           | 77         | Pass      |
| 4          | 3.0           | 69         | Fail      |
| 5          | 5.5           | 85         | Pass      |
| 6          | 2.0           | 62         | Fail      |
| 7          | 8.0           | 94         | Pass      |
| 8          | 3.5           | 73         | Fail      |
| 9          | 6.5           | 87         | Pass      |
| 10         | 1.5           | 58         | Fail      |
| 11         | 4.0           | 75         | Pass      |
| 12         | 6.0           | 84         | Pass      |
| 13         | 2.0           | 65         | Fail      |
| 14         | 7.5           | 93         | Pass      |
| 15         | 3.5           | 72         | Fail      |
| 16         | 5.0           | 81         | Pass      |
| 17         | 1.0           | 52         | Fail      |
| 18         | 4.5           | 76         | Pass      |
| 19         | 2.5           | 67         | Fail      |
| 20         | 7.0           | 88         | Pass      |
| 21         | 3.0           | 70         | Fail      |
| 22         | 6.0           | 86         | Pass      |
| 23         | 2.5           | 66         | Fail      |
| 24         | 5.5           | 82         | Pass      |
| 25         | 1.5           | 59         | Fail      |
| 26         | 4.0           | 74         | Fail      |
| 27         | 8.5           | 95         | Pass      |
| 28         | 3.0           | 71         | Fail      |
| 29         | 5.0           | 80         | Pass      |
| 30         | 7.5           | 91         | Pass      |
| 31         | 3.2           | 71         | Fail      |
| 32         | 6.8           | 88         | Pass      |
| 33         | 2.8           | 69         | Fail      |
| 34         | 4.8           | 79         | Pass      |
| 35         | 3.7           | 72         | Fail      |
| 36         | 5.3           | 83         | Pass      |
| 37         | 1.8           | 61         | Fail      |
| 38         | 7.3           | 90         | Pass      |
| 39         | 2.7           | 68         | Fail      |
| 40         | 4.7           | 78         | Pass      |
| 41         | 3.9           | 74         | Fail      |
| 42         | 6.2           | 86         | Pass      |
| 43         | 2.3           | 64         | Fail      |
| 44         | 5.8           | 84         | Pass      |
| 45         | 3.4           | 73         | Fail      |
| 46         | 4.9           | 79         | Pass      |
| 47         | 2.1           | 63         | Fail      |
| 48         | 7.8           | 92         | Pass      |
| 49         | 3.6           | 74         | Fail      |
| 50         | 5.2           | 82         | Pass      |

We'll use this dataset to build a decision tree model that predicts whether a student will pass or fail based on their study hours.

## Building the Decision Tree

To construct our tree, we'll recursively split the data based on the Gini impurity criterion. Let's first split our dataset into training (60%), validation (20%), and testing (20%) sets:

**Training Set**: Students 1-30 (30 students)
**Validation Set**: Students 31-40 (10 students)
**Testing Set**: Students 41-50 (10 students)

### Finding the Best Split

We need to evaluate different thresholds on "Hours Studied" to find the optimal split that minimizes Gini impurity. Looking at the unique values in our training set, we'll calculate Gini impurity for each potential split point.

For the entire training set (30 students):
- 14 Pass, 16 Fail
- $p_{Pass} = \frac{14}{30} = 0.467$
- $p_{Fail} = \frac{16}{30} = 0.533$

$$Gini_{root} = 1 - ((0.467)^2 + (0.533)^2) = 1 - (0.218 + 0.284) = 0.498$$

Let's calculate Gini impurity for a split at "Hours Studied ≤ 4.0":

**Left Node (Hours Studied ≤ 4.0)**:
- Total: 16 students
- Pass: 1 student
- Fail: 15 students
- $p_{Pass} = \frac{1}{16} = 0.063$
- $p_{Fail} = \frac{15}{16} = 0.937$

$$Gini_{left} = 1 - ((0.063)^2 + (0.937)^2) = 1 - (0.004 + 0.878) = 0.118$$

**Right Node (Hours Studied > 4.0)**:
- Total: 14 students
- Pass: 13 students
- Fail: 1 student
- $p_{Pass} = \frac{13}{14} = 0.929$
- $p_{Fail} = \frac{1}{14} = 0.071$

$$Gini_{right} = 1 - ((0.929)^2 + (0.071)^2) = 1 - (0.863 + 0.005) = 0.132$$

**Weighted Gini after split**:

$$Gini_{split} = \frac{16}{30} \times 0.118 + \frac{14}{30} \times 0.132 = 0.063 + 0.062 = 0.125$$

**Gini Gain**:

$$Gini\ Gain = 0.498 - 0.125 = 0.373$$

This is a substantial reduction in impurity, indicating that "Hours Studied ≤ 4.0" is a good splitting criterion. After evaluating all possible split points, we find this is indeed the optimal first split.

### The Complete Basic Decision Tree

Our basic decision tree looks like this:

```
Root: Hours Studied ≤ 4.0?
├── Yes:
│   └── Prediction: FAIL
└── No:
    └── Prediction: PASS
```

This simple tree achieves good separation of our classes with just one split.

### The Complex Decision Tree

Now, let's construct a more complex tree by adding additional features and allowing more splits:

```
Root: Hours Studied ≤ 4.0?
├── Yes:
│   ├── Hours Studied ≤ 2.0?
│   │   ├── Yes: Prediction: FAIL
│   │   └── No: 
│   │       ├── Hours Studied ≤ 3.0?
│   │       │   ├── Yes: Prediction: FAIL
│   │       │   └── No:
│   │       │       ├── Exam Score ≤ 73.5?
│   │       │       │   ├── Yes: Prediction: FAIL
│   │       │       │   └── No: Prediction: PASS
└── No:
    ├── Hours Studied ≤ 6.0?
    │   ├── Yes: 
    │   │   ├── Hours Studied ≤ 5.0?
    │   │   │   ├── Yes: Prediction: PASS
    │   │   │   └── No: Prediction: PASS
    │   │   └── Prediction: PASS
    │   └── No: Prediction: PASS
```

The complex tree has multiple levels of splits, creating a more detailed decision boundary.

## Evaluating Both Models on the Validation Set

Let's evaluate both the complex tree and the pruned tree (depth=1) on our validation set (students 31-40).

### Validation Set Results

| Student ID | Hours Studied | Exam Score | Actual | Complex Tree Prediction | Pruned Tree Prediction |
|------------|---------------|------------|--------|-------------------------|------------------------|
| 31         | 3.2           | 71         | Fail   | Fail                    | Fail                   |
| 32         | 6.8           | 88         | Pass   | Pass                    | Pass                   |
| 33         | 2.8           | 69         | Fail   | Fail                    | Fail                   |
| 34         | 4.8           | 79         | Pass   | Pass                    | Pass                   |
| 35         | 3.7           | 72         | Fail   | Fail                    | Fail                   |
| 36         | 5.3           | 83         | Pass   | Pass                    | Pass                   |
| 37         | 1.8           | 61         | Fail   | Fail                    | Fail                   |
| 38         | 7.3           | 90         | Pass   | Pass                    | Pass                   |
| 39         | 2.7           | 68         | Fail   | Fail                    | Fail                   |
| 40         | 4.7           | 78         | Pass   | Pass                    | Pass                   |

### Confusion Matrix for Complex Tree (Validation Set)

|                  | Predicted: Pass | Predicted: Fail |
|------------------|-----------------|-----------------|
| **Actual: Pass** | 5 (TP)          | 0 (FN)          |
| **Actual: Fail** | 0 (FP)          | 5 (TN)          |

### Confusion Matrix for Pruned Tree (Validation Set)

|                  | Predicted: Pass | Predicted: Fail |
|------------------|-----------------|-----------------|
| **Actual: Pass** | 5 (TP)          | 0 (FN)          |
| **Actual: Fail** | 0 (FP)          | 5 (TN)          |

### Metrics Calculations for Complex Tree (Validation Set)

**Accuracy**:
$$\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN} = \frac{5 + 5}{5 + 5 + 0 + 0} = \frac{10}{10} = 1.0 \text{ or } 100\%$$

**Precision**:
$$\text{Precision} = \frac{TP}{TP + FP} = \frac{5}{5 + 0} = \frac{5}{5} = 1.0 \text{ or } 100\%$$

**Recall** (Sensitivity):
$$\text{Recall} = \frac{TP}{TP + FN} = \frac{5}{5 + 0} = \frac{5}{5} = 1.0 \text{ or } 100\%$$

**F1 Score**:
$$\text{F1} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} = 2 \times \frac{1.0 \times 1.0}{1.0 + 1.0} = 2 \times \frac{1.0}{2.0} = 1.0 \text{ or } 100\%$$

### Metrics Calculations for Pruned Tree (Validation Set)

**Accuracy**:
$$\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN} = \frac{5 + 5}{5 + 5 + 0 + 0} = \frac{10}{10} = 1.0 \text{ or } 100\%$$

**Precision**:
$$\text{Precision} = \frac{TP}{TP + FP} = \frac{5}{5 + 0} = \frac{5}{5} = 1.0 \text{ or } 100\%$$

**Recall** (Sensitivity):
$$\text{Recall} = \frac{TP}{TP + FN} = \frac{5}{5 + 0} = \frac{5}{5} = 1.0 \text{ or } 100\%$$

**F1 Score**:
$$\text{F1} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} = 2 \times \frac{1.0 \times 1.0}{1.0 + 1.0} = 2 \times \frac{1.0}{2.0} = 1.0 \text{ or } 100\%$$

### Validation Set Performance Comparison

| Model | Accuracy | Precision | Recall | F1 Score |
|-------|----------|-----------|--------|----------|
| Complex Tree | 100% | 100% | 100% | 100% |
| Pruned Tree (max depth=1) | 100% | 100% | 100% | 100% |

Both models perform perfectly on the validation set for our dataset. This suggests that the simpler, pruned tree is capturing the essential pattern just as well as the more complex tree.

## Evaluating Both Models on the Test Set

Let's now evaluate both models on our test set (students 41-50) to see how they generalize to completely new data.

### Test Set Results

| Student ID | Hours Studied | Exam Score | Actual | Complex Tree Prediction | Pruned Tree Prediction |
|------------|---------------|------------|--------|-------------------------|------------------------|
| 41         | 3.9           | 74         | Fail   | Pass                    | Fail                   |
| 42         | 6.2           | 86         | Pass   | Pass                    | Pass                   |
| 43         | 2.3           | 64         | Fail   | Fail                    | Fail                   |
| 44         | 5.8           | 84         | Pass   | Pass                    | Pass                   |
| 45         | 3.4           | 73         | Fail   | Fail                    | Fail                   |
| 46         | 4.9           | 79         | Pass   | Pass                    | Pass                   |
| 47         | 2.1           | 63         | Fail   | Fail                    | Fail                   |
| 48         | 7.8           | 92         | Pass   | Pass                    | Pass                   |
| 49         | 3.6           | 74         | Fail   | Pass                    | Fail                   |
| 50         | 5.2           | 82         | Pass   | Pass                    | Pass                   |

### Confusion Matrix for Complex Tree (Test Set)

|                  | Predicted: Pass | Predicted: Fail |
|------------------|-----------------|-----------------|
| **Actual: Pass** | 5 (TP)          | 0 (FN)          |
| **Actual: Fail** | 2 (FP)          | 3 (TN)          |

### Confusion Matrix for Pruned Tree (Test Set)

|                  | Predicted: Pass | Predicted: Fail |
|------------------|-----------------|-----------------|
| **Actual: Pass** | 5 (TP)          | 0 (FN)          |
| **Actual: Fail** | 0 (FP)          | 5 (TN)          |

### Metrics Calculations for Complex Tree (Test Set)

**Accuracy**:
$$\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN} = \frac{5 + 3}{5 + 3 + 2 + 0} = \frac{8}{10} = 0.8 \text{ or } 80\%$$

**Precision**:
$$\text{Precision} = \frac{TP}{TP + FP} = \frac{5}{5 + 2} = \frac{5}{7} = 0.714 \text{ or } 71\%$$

**Recall** (Sensitivity):
$$\text{Recall} = \frac{TP}{TP + FN} = \frac{5}{5 + 0} = \frac{5}{5} = 1.0 \text{ or } 100\%$$

**F1 Score**:
$$\text{F1} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} = 2 \times \frac{0.714 \times 1.0}{0.714 + 1.0} = 2 \times \frac{0.714}{1.714} = 0.833 \text{ or } 83\%$$

### Metrics Calculations for Pruned Tree (Test Set)

**Accuracy**:
$$\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN} = \frac{5 + 5}{5 + 5 + 0 + 0} = \frac{10}{10} = 1.0 \text{ or } 100\%$$

**Precision**:
$$\text{Precision} = \frac{TP}{TP + FP} = \frac{5}{5 + 0} = \frac{5}{5} = 1.0 \text{ or } 100\%$$

**Recall** (Sensitivity):
$$\text{Recall} = \frac{TP}{TP + FN} = \frac{5}{5 + 0} = \frac{5}{5} = 1.0 \text{ or } 100\%$$

**F1 Score**:
$$\text{F1} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} = 2 \times \frac{1.0 \times 1.0}{1.0 + 1.0} = 2 \times \frac{1.0}{2.0} = 1.0 \text{ or } 100\%$$

### Test Set Performance Comparison

| Model | Accuracy | Precision | Recall | F1 Score |
|-------|----------|-----------|--------|----------|
| Complex Tree | 80% | 71% | 100% | 83% |
| Pruned Tree (max depth=1) | 100% | 100% | 100% | 100% |

## Why the Pruned Tree Outperforms the Complex Tree

The test set results reveal a significant insight: the pruned tree actually outperforms the more complex tree on new data! This demonstrates a fundamental concept in machine learning called overfitting.

The complex tree makes two critical mistakes on the test set:
1. For student #41 (Hours Studied = 3.9, Exam Score = 74, Actual = Fail), the complex tree predicts "Pass"
2. For student #49 (Hours Studied = 3.6, Exam Score = 74, Actual = Fail), the complex tree predicts "Pass"

Why does this happen? The complex tree creates very specific decision boundaries based on the training data that don't generalize well to new data. In contrast, the pruned tree with its simple rule (Hours Studied ≤ 4.0) captures the fundamental pattern in the data.

## The Bias-Variance Tradeoff

This example perfectly illustrates the bias-variance tradeoff:

- **Complex Tree**: Low bias (fits training data well), high variance (sensitive to small changes in data)
- **Pruned Tree**: Slightly higher bias, much lower variance (more stable across different datasets)

In this case, the pruned tree's balance of bias and variance leads to superior performance on new, unseen data. The complex tree has memorized specific patterns in the training data that don't generalize.

## Model Interpretability

Another major advantage of the pruned tree is interpretability. The decision rule is simple and clear:
- If a student studies 4.0 hours or less, predict "Fail"
- If a student studies more than 4.0 hours, predict "Pass"

This rule is easy to communicate and understand, making it valuable for explaining predictions to stakeholders.

## Statistical Significance

The 20% accuracy improvement of the pruned tree over the complex tree on the test set (100% vs. 80%) is substantial. This difference reinforces the principle of parsimony in model building: simpler models often generalize better than complex ones.

## Best Practices for Decision Tree Pruning

Based on our findings, here are some best practices for decision tree pruning:

1. **Always validate on a separate test set**: Performance on training data is not a reliable indicator of real-world performance
2. **Start with simpler models**: Begin with a shallow tree and increase complexity only if needed
3. **Use cross-validation**: Employ k-fold cross-validation to find the optimal tree depth
4. **Consider multiple metrics**: Look at precision, recall, and F1 score in addition to accuracy
5. **Be wary of perfect training performance**: If your model fits the training data too well, it might be overfitting

## Conclusion

Our analysis demonstrates a powerful lesson in machine learning: simpler models often outperform complex ones when it comes to generalization. The pruned decision tree with just one split achieved perfect performance on our test set, while the more complex tree made errors due to overfitting.

In our example, the pruned tree achieved:
- 20% higher accuracy (100% vs. 80%)
- 29% better precision (100% vs. 71%)
- Equal recall (100% vs. 100%)
- 17% higher F1 score (100% vs. 83%)

When building your own decision tree models, remember that the goal is not to perfectly fit the training data but to create a model that makes reliable predictions on new, unseen data. Pre-pruning techniques like limiting tree depth are powerful tools for creating more generalizable models.

By understanding the mathematical foundations of decision trees, applying appropriate evaluation metrics, and using pruning techniques, you can build models that strike the optimal balance between complexity and generalizability.
